{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03549b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Job Location, Company Name, Experience Required]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_shine_jobs(job_title, location, num_jobs=10):\n",
    "    # Step 1: Get the webpage\n",
    "    url = 'https://www.shine.com/'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Enter job title and location, then click the search button\n",
    "        data = {\n",
    "            'q': job_title,\n",
    "            'l': location\n",
    "        }\n",
    "        response = requests.post(url, data=data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Step 3: Scrape the data for the first 10 jobs\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            job_listings = soup.find_all('li', class_='sjscl')\n",
    "            \n",
    "            # Initialize lists to store scraped data\n",
    "            job_titles = []\n",
    "            job_locations = []\n",
    "            company_names = []\n",
    "            experience_required = []\n",
    "            \n",
    "            # Iterate through job listings\n",
    "            for job in job_listings[:num_jobs]:\n",
    "                job_title = job.find('h2').text.strip()\n",
    "                job_location = job.find('span', class_='loc').text.strip()\n",
    "                company_name = job.find('span', class_='sbc').text.strip()\n",
    "                exp = job.find('span', class_='exp').text.strip()\n",
    "                \n",
    "                # Append data to respective lists\n",
    "                job_titles.append(job_title)\n",
    "                job_locations.append(job_location)\n",
    "                company_names.append(company_name)\n",
    "                experience_required.append(exp)\n",
    "                \n",
    "            # Step 4: Create a dataframe of the scraped data\n",
    "            job_data = {\n",
    "                'Job Title': job_titles,\n",
    "                'Job Location': job_locations,\n",
    "                'Company Name': company_names,\n",
    "                'Experience Required': experience_required\n",
    "            }\n",
    "            df = pd.DataFrame(job_data)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Failed to fetch search results. Status code:\", response.status_code)\n",
    "    else:\n",
    "        print(\"Failed to fetch Shine.com homepage. Status code:\", response.status_code)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    job_title = \"Data Analyst\"\n",
    "    location = \"Bangalore\"\n",
    "    df = scrape_shine_jobs(job_title, location)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0689ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Job Location, Company Name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_shine_jobs(job_title, location, num_jobs=10):\n",
    "    # Step 1: Get the webpage\n",
    "    url = 'https://www.shine.com/'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Enter job title and location, then click the search button\n",
    "        data = {\n",
    "            'q': job_title,\n",
    "            'l': location\n",
    "        }\n",
    "        response = requests.post(url, data=data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Step 3: Scrape the data for the first 10 jobs\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            job_listings = soup.find_all('li', class_='sjscl')\n",
    "            \n",
    "            # Initialize lists to store scraped data\n",
    "            job_titles = []\n",
    "            job_locations = []\n",
    "            company_names = []\n",
    "            \n",
    "            # Iterate through job listings\n",
    "            for job in job_listings[:num_jobs]:\n",
    "                job_title = job.find('h2').text.strip()\n",
    "                job_location = job.find('span', class_='loc').text.strip()\n",
    "                company_name = job.find('span', class_='sbc').text.strip()\n",
    "                \n",
    "                # Append data to respective lists\n",
    "                job_titles.append(job_title)\n",
    "                job_locations.append(job_location)\n",
    "                company_names.append(company_name)\n",
    "                \n",
    "            # Step 4: Create a dataframe of the scraped data\n",
    "            job_data = {\n",
    "                'Job Title': job_titles,\n",
    "                'Job Location': job_locations,\n",
    "                'Company Name': company_names\n",
    "            }\n",
    "            df = pd.DataFrame(job_data)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Failed to fetch search results. Status code:\", response.status_code)\n",
    "    else:\n",
    "        print(\"Failed to fetch Shine.com homepage. Status code:\", response.status_code)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    job_title = \"Data Scientist\"\n",
    "    location = \"Bangalore\"\n",
    "    df = scrape_shine_jobs(job_title, location)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf796c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86d0214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location or Salary filter checkbox not found.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_shine_jobs(job_title, location, num_jobs=10):\n",
    "    # Step 1: Get the webpage\n",
    "    url = 'https://www.shine.com/'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Enter job title and click the search button\n",
    "        data = {\n",
    "            'q': job_title,\n",
    "        }\n",
    "        response = requests.post(url, data=data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Step 3: Apply location and salary filters\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            location_filter = soup.find('input', id='location')\n",
    "            salary_filter = soup.find('input', id='ctc')\n",
    "            \n",
    "            # Check the location and salary checkboxes\n",
    "            if location_filter and salary_filter:\n",
    "                location_filter['checked'] = 'true'\n",
    "                salary_filter['checked'] = 'true'\n",
    "                \n",
    "                # Submit the form to apply filters\n",
    "                response = requests.post(url, data={'q': job_title, 'l': location})\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Step 4: Scrape the data for the first 10 jobs\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    job_listings = soup.find_all('li', class_='sjscl')\n",
    "                    \n",
    "                    # Initialize lists to store scraped data\n",
    "                    job_titles = []\n",
    "                    job_locations = []\n",
    "                    company_names = []\n",
    "                    \n",
    "                    # Iterate through job listings\n",
    "                    for job in job_listings[:num_jobs]:\n",
    "                        job_title = job.find('h2').text.strip()\n",
    "                        job_location = job.find('span', class_='loc').text.strip()\n",
    "                        company_name = job.find('span', class_='sbc').text.strip()\n",
    "                        \n",
    "                        # Append data to respective lists\n",
    "                        job_titles.append(job_title)\n",
    "                        job_locations.append(job_location)\n",
    "                        company_names.append(company_name)\n",
    "                    \n",
    "                    # Step 5: Create a dataframe of the scraped data\n",
    "                    job_data = {\n",
    "                        'Job Title': job_titles,\n",
    "                        'Job Location': job_locations,\n",
    "                        'Company Name': company_names\n",
    "                    }\n",
    "                    df = pd.DataFrame(job_data)\n",
    "                    return df\n",
    "                else:\n",
    "                    print(\"Failed to fetch search results after applying filters. Status code:\", response.status_code)\n",
    "            else:\n",
    "                print(\"Location or Salary filter checkbox not found.\")\n",
    "        else:\n",
    "            print(\"Failed to fetch search results. Status code:\", response.status_code)\n",
    "    else:\n",
    "        print(\"Failed to fetch Shine.com homepage. Status code:\", response.status_code)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    job_title = \"Data Scientist\"\n",
    "    location = \"Bangalore\"\n",
    "    df = scrape_shine_jobs(job_title, location)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5597e363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17576541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Brand, Product Description, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_sunglasses(num_listings=100):\n",
    "    # Step 1: Get the webpage\n",
    "    url = 'https://www.flipkart.com/'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Enter \"sunglasses\" in the search field and click the search icon\n",
    "        data = {\n",
    "            'q': 'sunglasses',\n",
    "        }\n",
    "        response = requests.get(url + 'search', params=data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Step 3: Scrape data for the first 100 listings\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            sunglasses_listings = soup.find_all('div', class_='_2kHMtA')\n",
    "            \n",
    "            # Initialize lists to store scraped data\n",
    "            brands = []\n",
    "            descriptions = []\n",
    "            prices = []\n",
    "            \n",
    "            # Iterate through listings\n",
    "            for listing in sunglasses_listings[:num_listings]:\n",
    "                brand = listing.find('div', class_='_2WkVRV').text.strip()\n",
    "                description = listing.find('a', class_='IRpwTa').text.strip()\n",
    "                price = listing.find('div', class_='_30jeq3').text.strip()\n",
    "                \n",
    "                # Append data to respective lists\n",
    "                brands.append(brand)\n",
    "                descriptions.append(description)\n",
    "                prices.append(price)\n",
    "                \n",
    "            # Step 4: Create a dataframe of the scraped data\n",
    "            sunglasses_data = {\n",
    "                'Brand': brands,\n",
    "                'Product Description': descriptions,\n",
    "                'Price': prices\n",
    "            }\n",
    "            df = pd.DataFrame(sunglasses_data)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Failed to fetch search results. Status code:\", response.status_code)\n",
    "    else:\n",
    "        print(\"Failed to fetch Flipkart homepage. Status code:\", response.status_code)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_flipkart_sunglasses(num_listings=100)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c50a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c483114e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     43\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 44\u001b[0m     df \u001b[38;5;241m=\u001b[39m scrape_flipkart_reviews(url, num_reviews\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df)\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mscrape_flipkart_reviews\u001b[1;34m(url, num_reviews)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Iterate through reviews\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m reviews_list[:num_reviews]:\n\u001b[1;32m---> 21\u001b[0m     rating \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_3LWZlK\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     22\u001b[0m     review_title \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_2-N8zT\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     23\u001b[0m     review_text \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt-ZTKy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_reviews(url, num_reviews=100):\n",
    "    # Step 1: Get the webpage\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Scrape data for the reviews\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        reviews_list = soup.find_all('div', class_='t-ZTKy')\n",
    "        \n",
    "        # Initialize lists to store scraped data\n",
    "        ratings = []\n",
    "        review_titles = []\n",
    "        review_texts = []\n",
    "        \n",
    "        # Iterate through reviews\n",
    "        for review in reviews_list[:num_reviews]:\n",
    "            rating = review.find('div', class_='_3LWZlK').text.strip()\n",
    "            review_title = review.find('p', class_='_2-N8zT').text.strip()\n",
    "            review_text = review.find('div', class_='t-ZTKy').text.strip()\n",
    "            \n",
    "            # Append data to respective lists\n",
    "            ratings.append(rating)\n",
    "            review_titles.append(review_title)\n",
    "            review_texts.append(review_text)\n",
    "        \n",
    "        # Step 3: Create a dataframe of the scraped data\n",
    "        reviews_data = {\n",
    "            'Rating': ratings,\n",
    "            'Review Title': review_titles,\n",
    "            'Review Text': review_texts\n",
    "        }\n",
    "        df = pd.DataFrame(reviews_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to fetch reviews page. Status code:\", response.status_code)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART'\n",
    "    df = scrape_flipkart_reviews(url, num_reviews=100)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287867f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed31090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch Amazon.in homepage. Status code: 503\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_amazon_laptops(num_laptops=10):\n",
    "    # Step 1: Get the webpage\n",
    "    url = 'https://www.amazon.in/'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Enter \"Laptop\" in the search field and click the search icon\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        search_form = soup.find('form', attrs={'action': '/s'})\n",
    "        search_input = search_form.find('input', attrs={'id': 'twotabsearchtextbox'})\n",
    "        search_input['value'] = 'Laptop'\n",
    "        search_button = search_form.find('input', attrs={'type': 'submit'})\n",
    "        response = requests.get(url + 's', params={'k': 'Laptop'})\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Step 3: Set CPU Type filter to \"Intel Core i7\"\n",
    "            cpu_filter = soup.find('span', text='Intel Core i7').parent\n",
    "            cpu_filter_url = url + cpu_filter['href']\n",
    "            response = requests.get(cpu_filter_url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Step 4: Scrape data for the first 10 laptops\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                laptop_listings = soup.find_all('div', class_='s-result-item')\n",
    "                \n",
    "                # Initialize lists to store scraped data\n",
    "                titles = []\n",
    "                ratings = []\n",
    "                prices = []\n",
    "                \n",
    "                # Iterate through laptop listings\n",
    "                for listing in laptop_listings[:num_laptops]:\n",
    "                    title = listing.find('h2').text.strip()\n",
    "                    rating = listing.find('span', class_='a-icon-alt').text.strip()\n",
    "                    price = listing.find('span', class_='a-price-whole').text.strip()\n",
    "                    \n",
    "                    # Append data to respective lists\n",
    "                    titles.append(title)\n",
    "                    ratings.append(rating)\n",
    "                    prices.append(price)\n",
    "                \n",
    "                # Step 5: Create a dataframe of the scraped data\n",
    "                laptops_data = {\n",
    "                    'Title': titles,\n",
    "                    'Ratings': ratings,\n",
    "                    'Price': prices\n",
    "                }\n",
    "                df = pd.DataFrame(laptops_data)\n",
    "                return df\n",
    "            else:\n",
    "                print(\"Failed to set CPU Type filter. Status code:\", response.status_code)\n",
    "        else:\n",
    "            print(\"Failed to fetch search results. Status code:\", response.status_code)\n",
    "    else:\n",
    "        print(\"Failed to fetch Amazon.in homepage. Status code:\", response.status_code)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_amazon_laptops(num_laptops=10)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790b7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5fe8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABHISHEK\\AppData\\Local\\Temp\\ipykernel_13460\\2834455529.py:12: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  top_quotes_link = soup.find('a', text='Top Quotes')['href']\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '/top_quotes.html': No scheme supplied. Perhaps you meant https:///top_quotes.html?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     51\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.azquotes.com/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 52\u001b[0m     df \u001b[38;5;241m=\u001b[39m scrape_top_quotes(url)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df)\n",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m, in \u001b[0;36mscrape_top_quotes\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     11\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m top_quotes_link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop Quotes\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(top_quotes_link)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Step 3: Scrape data for the top quotes\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[0;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[0;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    581\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:486\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    483\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    485\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 486\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[0;32m    487\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    488\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m    489\u001b[0m     files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[0;32m    490\u001b[0m     data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    491\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mjson,\n\u001b[0;32m    492\u001b[0m     headers\u001b[38;5;241m=\u001b[39mmerge_setting(\n\u001b[0;32m    493\u001b[0m         request\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, dict_class\u001b[38;5;241m=\u001b[39mCaseInsensitiveDict\n\u001b[0;32m    494\u001b[0m     ),\n\u001b[0;32m    495\u001b[0m     params\u001b[38;5;241m=\u001b[39mmerge_setting(request\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams),\n\u001b[0;32m    496\u001b[0m     auth\u001b[38;5;241m=\u001b[39mmerge_setting(auth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth),\n\u001b[0;32m    497\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mmerged_cookies,\n\u001b[0;32m    498\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[0;32m    499\u001b[0m )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_url(url, params)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:439\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;241m*\u001b[39me\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheme:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No scheme supplied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No host supplied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL '/top_quotes.html': No scheme supplied. Perhaps you meant https:///top_quotes.html?"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_top_quotes(url):\n",
    "    # Step 1: Get the webpage\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Click on TopQuotes\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        top_quotes_link = soup.find('a', text='Top Quotes')['href']\n",
    "        response = requests.get(top_quotes_link)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Step 3: Scrape data for the top quotes\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            quote_items = soup.find_all('div', class_='wrap-block')\n",
    "\n",
    "            # Initialize lists to store scraped data\n",
    "            quotes = []\n",
    "            authors = []\n",
    "            types = []\n",
    "\n",
    "            # Iterate through quote items\n",
    "            for item in quote_items:\n",
    "                quote = item.find('a', class_='title').text.strip()\n",
    "                author = item.find('div', class_='author').text.strip()\n",
    "                quote_type = item.find('div', class_='qti').text.strip()\n",
    "\n",
    "                # Append data to respective lists\n",
    "                quotes.append(quote)\n",
    "                authors.append(author)\n",
    "                types.append(quote_type)\n",
    "\n",
    "            # Create a dataframe of the scraped data\n",
    "            quotes_data = {\n",
    "                'Quote': quotes,\n",
    "                'Author': authors,\n",
    "                'Type Of Quote': types\n",
    "            }\n",
    "            df = pd.DataFrame(quotes_data)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Failed to fetch TopQuotes page. Status code:\", response.status_code)\n",
    "    else:\n",
    "        print(\"Failed to fetch azquotes.com homepage. Status code:\", response.status_code)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.azquotes.com/'\n",
    "    df = scrape_top_quotes(url)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcd1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572af884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
