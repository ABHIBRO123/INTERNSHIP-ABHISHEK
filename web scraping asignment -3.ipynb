{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68b95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05049118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    url = f\"https://www.amazon.in/s?k={product.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        products = soup.find_all(\"span\", {\"class\": \"a-size-medium a-color-base a-text-normal\"})\n",
    "        \n",
    "        if products:\n",
    "            print(f\"Products found for '{product}':\\n\")\n",
    "            for idx, item in enumerate(products, 1):\n",
    "                print(f\"{idx}. {item.text.strip()}\")\n",
    "        else:\n",
    "            print(f\"No products found for '{product}'.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Amazon.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search for on Amazon.in: \").strip()\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(product_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        brand = soup.find(\"a\", {\"id\": \"bylineInfo\"})\n",
    "        brand_name = brand.text.strip() if brand else \"-\"\n",
    "        product_title = soup.find(\"span\", {\"id\": \"productTitle\"}).text.strip()\n",
    "        price = soup.find(\"span\", {\"id\": \"priceblock_ourprice\"})\n",
    "        price = price.text.strip() if price else \"-\"\n",
    "        return_exchange = soup.find(\"div\", {\"id\": \"RETURNS_POLICY\"})\n",
    "        return_exchange = return_exchange.text.strip() if return_exchange else \"-\"\n",
    "        expected_delivery = soup.find(\"span\", {\"id\": \"ddmDeliveryMessage\"})\n",
    "        expected_delivery = expected_delivery.text.strip() if expected_delivery else \"-\"\n",
    "        availability = soup.find(\"div\", {\"id\": \"availability\"})\n",
    "        availability = availability.text.strip() if availability else \"-\"\n",
    "        return brand_name, product_title, price, return_exchange, expected_delivery, availability, product_url\n",
    "    else:\n",
    "        return \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"\n",
    "\n",
    "def search_amazon(product):\n",
    "    products_data = []\n",
    "    for page in range(1, 4):  # Scraping first 3 pages\n",
    "        url = f\"https://www.amazon.in/s?k={product.replace(' ', '+')}&page={page}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            products = soup.find_all(\"span\", {\"class\": \"a-size-medium a-color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57853853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def scrape_images(keyword, num_images):\n",
    "    driver = webdriver.Chrome(executable_path=\"path_to_chromedriver\") # Change this to your ChromeDriver path\n",
    "    driver.get(\"https://www.google.com/imghp?hl=en\")\n",
    "\n",
    "    search_box = driver.find_element_by_css_selector(\"input.gLFyf.gsfi\")\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Scroll to load more images\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Get the page source and close the driver\n",
    "    page_source = driver.page_source\n",
    "    driver.close()\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    img_tags = soup.find_all('img', class_='rg_i')\n",
    "\n",
    "    # Create a directory to save images\n",
    "    if not os.path.exists(keyword):\n",
    "        os.makedirs(keyword)\n",
    "\n",
    "    # Download images\n",
    "    for i, img_tag in enumerate(img_tags[:num_images], 1):\n",
    "        img_url = img_tag['src']\n",
    "        img_name = f\"{keyword}_{i}.jpg\"\n",
    "        img_path = os.path.join(keyword, img_name)\n",
    "        try:\n",
    "            response = requests.get(img_url)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded {img_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {img_name}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    num_images_per_keyword = 10\n",
    "\n",
    "    for keyword in keywords:\n",
    "        scrape_images(keyword, num_images_per_keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_smartphones(keyword):\n",
    "    url = f\"https://www.flipkart.com/search?q={keyword.replace(' ', '%20')}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "        data = []\n",
    "        for product in products:\n",
    "            brand_name = product.find('div', {'class': '_4rR01T'}).text\n",
    "            smartphone_name = product.find('a', {'class': 'IRpwTa'}).text\n",
    "            product_url = 'https://www.flipkart.com' + product.find('a', {'class': 'IRpwTa'})['href']\n",
    "\n",
    "            specifications = product.find_all('li', {'class': 'rgWa7D'})\n",
    "            specs_dict = {spec.find('span', {'class': 'IRpwTa'}).text: spec.find('span', {'class': '_2LYh3d'}).text\n",
    "                          for spec in specifications}\n",
    "\n",
    "            # Fill in missing details with '-'\n",
    "            for key in ['Colour', 'RAM', 'Storage(ROM)', 'Primary Camera', 'Secondary Camera',\n",
    "                        'Display Size', 'Battery Capacity']:\n",
    "                specs_dict.setdefault(key, '-')\n",
    "\n",
    "            price = product.find('div', {'class': '_30jeq3'}).text\n",
    "\n",
    "            data.append({\n",
    "                'Brand Name': brand_name,\n",
    "                'Smartphone Name': smartphone_name,\n",
    "                'Colour': specs_dict['Colour'],\n",
    "                'RAM': specs_dict['RAM'],\n",
    "                'Storage(ROM)': specs_dict['Storage(ROM)'],\n",
    "                'Primary Camera': specs_dict['Primary Camera'],\n",
    "                'Secondary Camera': specs_dict['Secondary Camera'],\n",
    "                'Display Size': specs_dict['Display Size'],\n",
    "                'Battery Capacity': specs_dict['Battery Capacity'],\n",
    "                'Price': price,\n",
    "                'Product URL': product_url\n",
    "            })\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Flipkart.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keyword = input(\"Enter the smartphone you want to search for on Flipkart: \").strip()\n",
    "    data = scrape_smartphones(keyword)\n",
    "\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(f\"{keyword}_smartphones.csv\", index=False)\n",
    "        print(f\"Data saved to {keyword}_smartphones.csv successfully.\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a97e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_coordinates(city):\n",
    "    # Launch the Chrome browser\n",
    "    driver = webdriver.Chrome(executable_path=\"path_to_chromedriver\") # Change this to your ChromeDriver path\n",
    "    driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "    # Find the search box, enter the city name, and submit the search\n",
    "    search_box = driver.find_element_by_css_selector(\"input.tactile-searchbox-input\")\n",
    "    search_box.send_keys(city)\n",
    "    search_box.send_keys(Keys.ENTER)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find the latitude and longitude from the page\n",
    "    try:\n",
    "        # Latitude and Longitude are stored in the URL of the map\n",
    "        map_url = soup.find('meta', {'itemprop': 'image'})['content']\n",
    "        lat_long_str = map_url.split('@')[1].split(',')[0:2]\n",
    "        latitude = float(lat_long_str[0])\n",
    "        longitude = float(lat_long_str[1])\n",
    "        return latitude, longitude\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the name of the city to get its coordinates: \")\n",
    "    latitude, longitude = scrape_coordinates(city)\n",
    "    if latitude is not None and longitude is not None:\n",
    "        print(f\"Coordinates of {city}: Latitude - {latitude}, Longitude - {longitude}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch coordinates for {city}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27011646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "        \n",
    "        data = []\n",
    "        for laptop in laptops:\n",
    "            name = laptop.find('div', class_='TopNumbeHeading sticky-footer').text.strip()\n",
    "            specs = laptop.find_all('div', class_='right-container')\n",
    "            processor = specs[0].text.strip()\n",
    "            memory = specs[1].text.strip()\n",
    "            os = specs[2].text.strip()\n",
    "            display = specs[3].text.strip()\n",
    "            price = specs[4].text.strip()\n",
    "\n",
    "            data.append({\n",
    "                'Name': name,\n",
    "                'Processor': processor,\n",
    "                'Memory': memory,\n",
    "                'Operating System': os,\n",
    "                'Display': display,\n",
    "                'Price': price\n",
    "            })\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Failed to fetch data from digit.in.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_gaming_laptops()\n",
    "\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"best_gaming_laptops.csv\", index=False)\n",
    "        print(\"Data saved to best_gaming_laptops.csv successfully.\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6979219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        billionaires = soup.find_all('div', class_='personName')\n",
    "\n",
    "        data = []\n",
    "        for billionaire in billionaires:\n",
    "            rank = billionaire.find_previous('div', class_='rank').text.strip()\n",
    "            name = billionaire.text.strip()\n",
    "            net_worth = billionaire.find_next('div', class_='netWorth').text.strip()\n",
    "            age = billionaire.find_next('div', class_='age').text.strip()\n",
    "            citizenship = billionaire.find_next('div', class_='countryOfCitizenship').text.strip()\n",
    "            source = billionaire.find_next('div', class_='source-column').text.strip()\n",
    "            industry = billionaire.find_next('div', class_='category').text.strip()\n",
    "\n",
    "            data.append({\n",
    "                'Rank': rank,\n",
    "                'Name': name,\n",
    "                'Net Worth': net_worth,\n",
    "                'Age': age,\n",
    "                'Citizenship': citizenship,\n",
    "                'Source': source,\n",
    "                'Industry': industry\n",
    "            })\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Forbes.com.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_billionaires()\n",
    "\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"billionaires.csv\", index=False)\n",
    "        print(\"Data saved to billionaires.csv successfully.\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "\n",
    "def get_video_comments(video_id):\n",
    "    # Set up the YouTube Data API service\n",
    "    youtube = build('youtube', 'v3', developerKey='YOUR_API_KEY')\n",
    "\n",
    "    # Request comments for the specified video\n",
    "    response = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        maxResults=100,  # Max results per page (100 is the maximum allowed)\n",
    "        order='time'     # Order comments by time\n",
    "    ).execute()\n",
    "\n",
    "    comments = []\n",
    "    while response:\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_time = datetime.strptime(comment['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            comments.append({\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Upvotes': comment['likeCount'],\n",
    "                'Time': comment_time\n",
    "            })\n",
    "\n",
    "        # Check if there are more comments available\n",
    "        if 'nextPageToken' in response:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=response['nextPageToken'],\n",
    "                order='time'\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    video_id = input(\"Enter the YouTube video ID: \")\n",
    "    all_comments = get_video_comments(video_id)\n",
    "\n",
    "    if all_comments:\n",
    "        print(f\"Total comments extracted: {len(all_comments)}\")\n",
    "        for comment in all_comments:\n",
    "            print(f\"Comment: {comment['Comment']}\")\n",
    "            print(f\"Upvotes: {comment['Upvotes']}\")\n",
    "            print(f\"Time: {comment['Time']}\")\n",
    "            print(\"-------------------------------------\")\n",
    "    else:\n",
    "        print(\"No comments found for the given video ID.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cca472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
