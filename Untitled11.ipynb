{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a198e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Wikipedia Headers:\n",
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_wikipedia_headers(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        header_texts = [header.text.strip() for header in headers]\n",
    "        return header_texts\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(headers):\n",
    "    df = pd.DataFrame({'Headers': headers})\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wikipedia_url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "    \n",
    "    headers = get_wikipedia_headers(wikipedia_url)\n",
    "    \n",
    "    if headers:\n",
    "        df = create_dataframe(headers)\n",
    "        print(\"DataFrame with Wikipedia Headers:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No headers found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c73e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458b4a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve content. Status code: 404\n",
      "No data found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_former_presidents_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        president_table = soup.find('table', {'class': 'tablepress tablepress-id-13'})\n",
    "        \n",
    "        if president_table:\n",
    "            presidents_data = []\n",
    "            rows = president_table.find_all('tr')[1:]  # Skip the header row\n",
    "            for row in rows:\n",
    "                columns = row.find_all('td')\n",
    "                name = columns[0].text.strip()\n",
    "                term_of_office = columns[1].text.strip()\n",
    "                presidents_data.append({'Name': name, 'Term of Office': term_of_office})\n",
    "            return presidents_data\n",
    "        else:\n",
    "            print(\"Table not found on the page.\")\n",
    "            return []\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    presidents_url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "    \n",
    "    presidents_data = get_former_presidents_data(presidents_url)\n",
    "    \n",
    "    if presidents_data:\n",
    "        df = create_dataframe(presidents_data)\n",
    "        print(\"DataFrame with Former Presidents of India:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002a) Top 10 ODI teams in men’s cricket along with the recor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a88d8516",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     42\u001b[0m     odi_rankings_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.icc-cricket.com/rankings/mens/team-rankings/odi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 44\u001b[0m     odi_teams_data \u001b[38;5;241m=\u001b[39m scrape_odi_rankings(odi_rankings_url)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m odi_teams_data:\n\u001b[0;32m     47\u001b[0m         df \u001b[38;5;241m=\u001b[39m create_dataframe(odi_teams_data)\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mscrape_odi_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Extracting team names and records\u001b[39;00m\n\u001b[0;32m     12\u001b[0m teams_container \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrankings-block__container\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 13\u001b[0m teams \u001b[38;5;241m=\u001b[39m teams_container\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable-body\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m team \u001b[38;5;129;01min\u001b[39;00m teams[:\u001b[38;5;241m10\u001b[39m]:\n\u001b[0;32m     16\u001b[0m     rank \u001b[38;5;241m=\u001b[39m team\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mranking-pos\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_odi_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        teams_data = []\n",
    "\n",
    "        # Extracting team names and records\n",
    "        teams_container = soup.find('div', {'class': 'rankings-block__container'})\n",
    "        teams = teams_container.find_all('div', {'class': 'table-body'})\n",
    "\n",
    "        for team in teams[:10]:\n",
    "            rank = team.find('span', {'class': 'ranking-pos'}).text.strip()\n",
    "            name = team.find('span', {'class': 'u-hide-phablet'}).text.strip()\n",
    "            matches = team.find_all('td')[2].text.strip()\n",
    "            points = team.find_all('td')[3].text.strip()\n",
    "            rating = team.find_all('td')[4].text.strip()\n",
    "\n",
    "            record = {\n",
    "                'Rank': rank,\n",
    "                'Team': name,\n",
    "                'Matches': matches,\n",
    "                'Points': points,\n",
    "                'Rating': rating\n",
    "            }\n",
    "\n",
    "            teams_data.append(record)\n",
    "\n",
    "        return teams_data\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    odi_rankings_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "    \n",
    "    odi_teams_data = scrape_odi_rankings(odi_rankings_url)\n",
    "    \n",
    "    if odi_teams_data:\n",
    "        df = create_dataframe(odi_teams_data)\n",
    "        print(\"DataFrame with Top 10 ODI Teams in Men's Cricket:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2130ad",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\u0002i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864ab0f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     40\u001b[0m     ai_articles_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 42\u001b[0m     ai_articles_data \u001b[38;5;241m=\u001b[39m scrape_most_downloaded_articles(ai_articles_url)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ai_articles_data:\n\u001b[0;32m     45\u001b[0m         df \u001b[38;5;241m=\u001b[39m create_dataframe(ai_articles_data)\n",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m, in \u001b[0;36mscrape_most_downloaded_articles\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Extracting article details\u001b[39;00m\n\u001b[0;32m     12\u001b[0m articles_container \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpod-listing\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 13\u001b[0m articles \u001b[38;5;241m=\u001b[39m articles_container\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpod-listing-item\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[0;32m     16\u001b[0m     title \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpod-listing-header\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles_data = []\n",
    "\n",
    "        # Extracting article details\n",
    "        articles_container = soup.find('div', {'class': 'pod-listing'})\n",
    "        articles = articles_container.find_all('li', {'class': 'pod-listing-item'})\n",
    "\n",
    "        for article in articles:\n",
    "            title = article.find('a', {'class': 'pod-listing-header'}).text.strip()\n",
    "            authors = article.find('span', {'class': 'pod-listing-authors'}).text.strip()\n",
    "            published_date = article.find('span', {'class': 'pod-listing-date'}).text.strip()\n",
    "            paper_url = article.find('a', {'class': 'pod-listing-header'})['href']\n",
    "\n",
    "            record = {\n",
    "                'Paper Title': title,\n",
    "                'Authors': authors,\n",
    "                'Published Date': published_date,\n",
    "                'Paper URL': paper_url\n",
    "            }\n",
    "\n",
    "            articles_data.append(record)\n",
    "\n",
    "        return articles_data\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ai_articles_url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "    \n",
    "    ai_articles_data = scrape_most_downloaded_articles(ai_articles_url)\n",
    "    \n",
    "    if ai_articles_data:\n",
    "        df = create_dataframe(ai_articles_data)\n",
    "        print(\"DataFrame with Most Downloaded AI Articles:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19157a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\u0002i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6031383f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 48) (79003069.py, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 48\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"Data\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 48)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        restaurants_data = []\n",
    "\n",
    "        # Extracting restaurant details\n",
    "        restaurants_container = soup.find('div', {'class': 'listing-sec'})\n",
    "        restaurants = restaurants_container.find_all('div', {'class': 'restnt-info'})\n",
    "\n",
    "        for restaurant in restaurants:\n",
    "            name = restaurant.find('h2', {'itemprop': 'name'}).text.strip()\n",
    "            cuisine = restaurant.find('span', {'itemprop': 'servesCuisine'}).text.strip()\n",
    "            location = restaurant.find('span', {'itemprop': 'addressLocality'}).text.strip()\n",
    "            ratings = restaurant.find('span', {'itemprop': 'ratingValue'}).text.strip()\n",
    "            image_url = restaurant.find('img')['src']\n",
    "\n",
    "            record = {\n",
    "                'Restaurant Name': name,\n",
    "                'Cuisine': cuisine,\n",
    "                'Location': location,\n",
    "                'Ratings': ratings,\n",
    "                'Image URL': image_url\n",
    "            }\n",
    "\n",
    "            restaurants_data.append(record)\n",
    "\n",
    "        return restaurants_data\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dineout_url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "    \n",
    "    dineout_data = scrape_dineout_details(dineout_url)\n",
    "    \n",
    "    if dineout_data:\n",
    "        df = create_dataframe(dineout_data)\n",
    "        print(\"Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5a09e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     42\u001b[0m     dineout_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.dineout.co.in/delhi-restaurants/buffet-special\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 44\u001b[0m     dineout_data \u001b[38;5;241m=\u001b[39m scrape_dineout_details(dineout_url)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dineout_data:\n\u001b[0;32m     47\u001b[0m         df \u001b[38;5;241m=\u001b[39m create_dataframe(dineout_data)\n",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m, in \u001b[0;36mscrape_dineout_details\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Extracting restaurant details\u001b[39;00m\n\u001b[0;32m     12\u001b[0m restaurants_container \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlisting-sec\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 13\u001b[0m restaurants \u001b[38;5;241m=\u001b[39m restaurants_container\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrestnt-info\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m restaurant \u001b[38;5;129;01min\u001b[39;00m restaurants:\n\u001b[0;32m     16\u001b[0m     name \u001b[38;5;241m=\u001b[39m restaurant\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitemprop\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        restaurants_data = []\n",
    "\n",
    "        # Extracting restaurant details\n",
    "        restaurants_container = soup.find('div', {'class': 'listing-sec'})\n",
    "        restaurants = restaurants_container.find_all('div', {'class': 'restnt-info'})\n",
    "\n",
    "        for restaurant in restaurants:\n",
    "            name = restaurant.find('h2', {'itemprop': 'name'}).text.strip()\n",
    "            cuisine = restaurant.find('span', {'itemprop': 'servesCuisine'}).text.strip()\n",
    "            location = restaurant.find('span', {'itemprop': 'addressLocality'}).text.strip()\n",
    "            ratings = restaurant.find('span', {'itemprop': 'ratingValue'}).text.strip()\n",
    "            image_url = restaurant.find('img')['src']\n",
    "\n",
    "            record = {\n",
    "                'Restaurant Name': name,\n",
    "                'Cuisine': cuisine,\n",
    "                'Location': location,\n",
    "                'Ratings': ratings,\n",
    "                'Image URL': image_url\n",
    "            }\n",
    "\n",
    "            restaurants_data.append(record)\n",
    "\n",
    "        return restaurants_data\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dineout_url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "    \n",
    "    dineout_data = scrape_dineout_details(dineout_url)\n",
    "    \n",
    "    if dineout_data:\n",
    "        df = create_dataframe(dineout_data)\n",
    "        print(\"DataFrame with Dineout Restaurant Details:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1692510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
