{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f365960",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5ee8efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred:\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81693d58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mui\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriverWait\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Launching Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Loading the BCCI website\n",
    "    driver.get(\"https://www.bcci.tv/\")\n",
    "    \n",
    "    # Finding and clicking on the 'International' dropdown menu\n",
    "    international_dropdown = WebDriverWait(driver, 10).until(\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0540996",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6717c041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/3f/fd/c2e7bb547b5b96c7bd536b4a80c4564b7ce5cd38d10095fbba8648996ab9/selenium-4.18.1-py3-none-any.whl.metadata\n",
      "  Downloading selenium-4.18.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Obtaining dependency information for trio~=0.17 from https://files.pythonhosted.org/packages/14/fb/9299cf74953f473a15accfdbe2c15218e766bae8c796f2567c83bae03e98/trio-0.24.0-py3-none-any.whl.metadata\n",
      "  Downloading trio-0.24.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Obtaining dependency information for trio-websocket~=0.9 from https://files.pythonhosted.org/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl.metadata\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Collecting typing_extensions>=4.9.0 (from selenium)\n",
      "  Obtaining dependency information for typing_extensions>=4.9.0 from https://files.pythonhosted.org/packages/f9/de/dc04a3ea60b22624b51c703a84bbe0184abcd1d0b9bc8074b5d6b7ab90bb/typing_extensions-4.10.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Obtaining dependency information for outcome from https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl.metadata\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Obtaining dependency information for sniffio>=1.3.0 from https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Obtaining dependency information for wsproto>=0.14 from https://files.pythonhosted.org/packages/78/58/e860788190eba3bcce367f74d29c4675466ce8dddfba85f7827588416f01/wsproto-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Obtaining dependency information for h11<1,>=0.9.0 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.18.1-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/10.0 MB 12.6 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.8/10.0 MB 10.0 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.8/10.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.4/10.0 MB 10.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.9/10.0 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.2/10.0 MB 10.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.7/10.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.3/10.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.7/10.0 MB 10.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.2/10.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.5/10.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.2/10.0 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.7/10.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.1/10.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/10.0 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.0/10.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.6/10.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.1/10.0 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/10.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading trio-0.24.0-py3-none-any.whl (460 kB)\n",
      "   ---------------------------------------- 0.0/460.2 kB ? eta -:--:--\n",
      "   --------------------------------------  450.6/460.2 kB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 460.2/460.2 kB 9.8 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: typing_extensions, sniffio, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.18.1 sniffio-1.3.1 trio-0.24.0 trio-websocket-0.11.1 typing_extensions-4.10.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solutions:--\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877d14b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "856f5b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "try:\n",
    "    # Sending a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Parsing the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Finding the table containing the required data\n",
    "    table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "    \n",
    "    # Initializing lists to store data\n",
    "    ranks = []\n",
    "    names = []\n",
    "    artists = []\n",
    "    upload_dates = []\n",
    "    views = []\n",
    "    \n",
    "    # Extracting data from each row of the table\n",
    "    for row in table.find_all(\"tr\")[1:]: # Skipping the header row\n",
    "        cols = row.find_all(\"td\")\n",
    "        ranks.append(cols[0].text.strip())\n",
    "        names.append(cols[1].text.strip())\n",
    "        artists.append(cols[2].text.strip())\n",
    "        upload_dates.append(cols[3].text.strip())\n",
    "        views.append(cols[4].text.strip())\n",
    "    \n",
    "    # Printing the scraped data\n",
    "    print(\"Rank\\tName\\tArtist\\tUpload Date\\tViews\")\n",
    "    for rank, name, artist, upload_date, view in zip(ranks, names, artists, upload_dates, views):\n",
    "        print(f\"{rank}\\t{name}\\t{artist}\\t{upload_date}\\t{view}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beece213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error sending stats to Plausible: error sending request for url (https://plausible.io/api/event): operation timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=122.0.6261.95)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF71DBCAD22+56930]\n",
      "\t(No symbol) [0x00007FF71DB3F622]\n",
      "\t(No symbol) [0x00007FF71D9F42E5]\n",
      "\t(No symbol) [0x00007FF71D9D1D4C]\n",
      "\t(No symbol) [0x00007FF71DA623F7]\n",
      "\t(No symbol) [0x00007FF71DA77891]\n",
      "\t(No symbol) [0x00007FF71DA5BA43]\n",
      "\t(No symbol) [0x00007FF71DA2D438]\n",
      "\t(No symbol) [0x00007FF71DA2E4D1]\n",
      "\tGetHandleVerifier [0x00007FF71DF46AAD+3709933]\n",
      "\tGetHandleVerifier [0x00007FF71DF9FFED+4075821]\n",
      "\tGetHandleVerifier [0x00007FF71DF9817F+4043455]\n",
      "\tGetHandleVerifier [0x00007FF71DC69756+706710]\n",
      "\t(No symbol) [0x00007FF71DB4B8FF]\n",
      "\t(No symbol) [0x00007FF71DB46AE4]\n",
      "\t(No symbol) [0x00007FF71DB46C3C]\n",
      "\t(No symbol) [0x00007FF71DB368F4]\n",
      "\tBaseThreadInitThunk [0x00007FFD6DF5257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFD6F6EAA58+40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Launching Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Loading the BCCI website\n",
    "    driver.get(\"https://www.bcci.tv/\")\n",
    "    \n",
    "    # Finding and clicking on the 'International' dropdown menu\n",
    "    international_dropdown = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"li.nav__item--international\"))\n",
    "    )\n",
    "    international_dropdown.click()\n",
    "    \n",
    "    # Finding and clicking on the 'Fixtures' link under 'International' dropdown\n",
    "    fixtures_link = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//a[text()='Fixtures']\"))\n",
    "    )\n",
    "    fixtures_link.click()\n",
    "    \n",
    "    # Waiting for the fixtures page to load completely\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"js-list\"))\n",
    "    )\n",
    "    \n",
    "    # Getting the page source after clicking on 'Fixtures' link\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Parsing the HTML content\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    \n",
    "    # Finding the fixture items\n",
    "    fixtures = soup.find_all(\"div\", class_=\"js-list__item fixture\")\n",
    "    \n",
    "    # Extracting required details from each fixture item\n",
    "    for fixture in fixtures:\n",
    "        series = fixture.find(\"span\", class_=\"fixture__format-strip\").text.strip()\n",
    "        place = fixture.find(\"p\", class_=\"fixture__additional-info\").text.strip()\n",
    "        date = fixture.find(\"span\", class_=\"fixture__date\").text.strip()\n",
    "        time = fixture.find(\"span\", class_=\"fixture__time\").text.strip()\n",
    "        \n",
    "        print(\"Series:\", series)\n",
    "        print(\"Place:\", place)\n",
    "        print(\"Date:\", date)\n",
    "        print(\"Time:\", time)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "finally:\n",
    "    # Closing the WebDriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2240238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF71DBCAD22+56930]\n",
      "\t(No symbol) [0x00007FF71DB3F622]\n",
      "\t(No symbol) [0x00007FF71D9F42E5]\n",
      "\t(No symbol) [0x00007FF71DA398ED]\n",
      "\t(No symbol) [0x00007FF71DA39A2C]\n",
      "\t(No symbol) [0x00007FF71DA7A967]\n",
      "\t(No symbol) [0x00007FF71DA5BCDF]\n",
      "\t(No symbol) [0x00007FF71DA781E2]\n",
      "\t(No symbol) [0x00007FF71DA5BA43]\n",
      "\t(No symbol) [0x00007FF71DA2D438]\n",
      "\t(No symbol) [0x00007FF71DA2E4D1]\n",
      "\tGetHandleVerifier [0x00007FF71DF46AAD+3709933]\n",
      "\tGetHandleVerifier [0x00007FF71DF9FFED+4075821]\n",
      "\tGetHandleVerifier [0x00007FF71DF9817F+4043455]\n",
      "\tGetHandleVerifier [0x00007FF71DC69756+706710]\n",
      "\t(No symbol) [0x00007FF71DB4B8FF]\n",
      "\t(No symbol) [0x00007FF71DB46AE4]\n",
      "\t(No symbol) [0x00007FF71DB46C3C]\n",
      "\t(No symbol) [0x00007FF71DB368F4]\n",
      "\tBaseThreadInitThunk [0x00007FFD6DF5257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFD6F6EAA58+40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Launching Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Loading the statisticstimes website\n",
    "    driver.get(\"http://statisticstimes.com/\")\n",
    "    \n",
    "    # Finding and clicking on the 'Economy' link\n",
    "    economy_link = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.LINK_TEXT, \"Economy\"))\n",
    "    )\n",
    "    economy_link.click()\n",
    "    \n",
    "    # Finding and clicking on the 'GDP of Indian states' link under 'GDP of Indian Economy' section\n",
    "    gdp_of_states_link = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.LINK_TEXT, \"GDP of Indian states\"))\n",
    "    )\n",
    "    gdp_of_states_link.click()\n",
    "    \n",
    "    # Waiting for the GDP of Indian states page to load completely\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"display\"))\n",
    "    )\n",
    "    \n",
    "    # Getting the page source after clicking on 'GDP of Indian states' link\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Parsing the HTML content\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    \n",
    "    # Finding the table containing the required data\n",
    "    table = soup.find(\"table\", {\"id\": \"table_id\"})\n",
    "    \n",
    "    # Extracting required details from each row of the table\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        cols = row.find_all(\"td\")\n",
    "        rank = cols[0].text.strip()\n",
    "        state = cols[1].text.strip()\n",
    "        gsdp_18_19_current = cols[2].text.strip()\n",
    "        gsdp_19_20_current = cols[3].text.strip()\n",
    "        share_18_19 = cols[4].text.strip()\n",
    "        gdp_billion = cols[5].text.strip()\n",
    "        \n",
    "        print(\"Rank:\", rank)\n",
    "        print(\"State:\", state)\n",
    "        print(\"GSDP(18-19) - Current Prices:\", gsdp_18_19_current)\n",
    "        print(\"GSDP(19-20) - Current Prices:\", gsdp_19_20_current)\n",
    "        print(\"Share(18-19):\", share_18_19)\n",
    "        print(\"GDP ($ billion):\", gdp_billion)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "finally:\n",
    "    # Closing the WebDriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef59f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6209da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'WebDriver' object has no attribute 'find_element_by_xpath'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Launching Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Loading the GitHub website\n",
    "    driver.get(\"https://github.com/\")\n",
    "    \n",
    "    # Finding and clicking on the 'Trending' link\n",
    "    trending_link = driver.find_element_by_xpath(\"//a[contains(text(),'Trending')]\")\n",
    "    trending_link.click()\n",
    "    \n",
    "    # Getting the page source after clicking on 'Trending' link\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Parsing the HTML content\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    \n",
    "    # Finding the trending repositories\n",
    "    repositories = soup.find_all(\"article\", class_=\"Box-row\")\n",
    "    \n",
    "    # Extracting required details from each trending repository\n",
    "    for repo in repositories:\n",
    "        repo_title = repo.find(\"h1\", class_=\"h3\").text.strip()\n",
    "        repo_desc = repo.find(\"p\", class_=\"col-9\").text.strip()\n",
    "        contributors_count = repo.find(\"a\", class_=\"muted-link\").text.strip().split()[0]\n",
    "        language_used = repo.find(\"span\", class_=\"d-inline-block\").text.strip()\n",
    "        \n",
    "        print(\"Repository Title:\", repo_title)\n",
    "        print(\"Repository Description:\", repo_desc)\n",
    "        print(\"Contributors Count:\", contributors_count)\n",
    "        print(\"Language Used:\", language_used)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "finally:\n",
    "    # Closing the WebDriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67132d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3836ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'NoneType' object has no attribute 'is_displayed'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Launching Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Loading the Billboard website\n",
    "    driver.get(\"https://www.billboard.com/\")\n",
    "    \n",
    "    # Finding and clicking on the 'Charts' option\n",
    "    charts_option = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//span[contains(text(),'Charts')]\"))\n",
    "    )\n",
    "    charts_option.click()\n",
    "    \n",
    "    # Finding and clicking on the 'Hot 100' link\n",
    "    hot_100_link = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//a[contains(text(),'Hot 100')]\"))\n",
    "    )\n",
    "    hot_100_link.click()\n",
    "    \n",
    "    # Waiting for the Hot 100 page to load completely\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"chart-list__elements\"))\n",
    "    )\n",
    "    \n",
    "    # Getting the page source after clicking on 'Hot 100' link\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Parsing the HTML content\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    \n",
    "    # Finding the chart list\n",
    "    chart_list = soup.find(\"ol\", class_=\"chart-list__elements\")\n",
    "    \n",
    "    # Extracting required details from each song in the chart list\n",
    "    for song in chart_list.find_all(\"li\"):\n",
    "        song_name = song.find(\"span\", class_=\"chart-element__information__song\").text.strip()\n",
    "        artist_name = song.find(\"span\", class_=\"chart-element__information__artist\").text.strip()\n",
    "        last_week_rank = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--last\").text.strip()\n",
    "        peak_rank = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--peak\").text.strip()\n",
    "        weeks_on_board = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--week\").text.strip()\n",
    "        \n",
    "        print(\"Song Name:\", song_name)\n",
    "        print(\"Artist Name:\", artist_name)\n",
    "        print(\"Last Week Rank:\", last_week_rank)\n",
    "        print(\"Peak Rank:\", peak_rank)\n",
    "        print(\"Weeks on Board:\", weeks_on_board)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "finally:\n",
    "    # Closing the WebDriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c2a78d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (4.12.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\abhishek\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d033ede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "try:\n",
    "    # Sending a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Parsing the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Finding the table containing the required data\n",
    "    table = soup.find(\"table\", class_=\"in-article sortable\")\n",
    "    \n",
    "    # Extracting required details from each row of the table\n",
    "    for row in table.find_all(\"tr\")[1:]: # Skipping the header row\n",
    "        cols = row.find_all(\"td\")\n",
    "        book_name = cols[1].text.strip()\n",
    "        author_name = cols[2].text.strip()\n",
    "        volumes_sold = cols[3].text.strip()\n",
    "        publisher = cols[4].text.strip()\n",
    "        genre = cols[5].text.strip()\n",
    "        \n",
    "        print(\"Book Name:\", book_name)\n",
    "        print(\"Author Name:\", author_name)\n",
    "        print(\"Volumes Sold:\", volumes_sold)\n",
    "        print(\"Publisher:\", publisher)\n",
    "        print(\"Genre:\", genre)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b88f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a9b52fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'NoneType' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find the link to the Show All Dataset page\n",
    "    show_all_link = soup.find(\"a\", href=\"datasets.html\")\n",
    "    # Construct the URL for the Show All Dataset page\n",
    "    show_all_url = url + show_all_link.get(\"href\")\n",
    "    \n",
    "    # Send a GET request to the Show All Dataset page\n",
    "    response = requests.get(show_all_url)\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find the table containing the dataset details\n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "    \n",
    "    # Extracting required details from each row of the table\n",
    "    for row in table.find_all(\"tr\")[1:]: # Skipping the header row\n",
    "        cols = row.find_all(\"td\")\n",
    "        dataset_name = cols[0].text.strip()\n",
    "        data_type = cols[1].text.strip()\n",
    "        task = cols[2].text.strip()\n",
    "        attribute_type = cols[3].text.strip()\n",
    "        num_instances = cols[4].text.strip()\n",
    "        num_attributes = cols[5].text.strip()\n",
    "        year = cols[6].text.strip()\n",
    "        \n",
    "        print(\"Dataset Name:\", dataset_name)\n",
    "        print(\"Data Type:\", data_type)\n",
    "        print(\"Task:\", task)\n",
    "        print(\"Attribute Type:\", attribute_type)\n",
    "        print(\"No of Instances:\", num_instances)\n",
    "        print(\"No of Attributes:\", num_attributes)\n",
    "        print(\"Year:\", year)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311e439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
